[{"content":"常用的神经网络层 全连接层 权重的数量：$c_i*c_o$，分别代表输入维度和输出维度\n卷积层 感受野 是指输出的一个像素由多少元素决定，越大说明涵盖的信息越全面\n参数 首先是一些概念，stride指卷积的步长，padding指边缘填充的元素数量，pooling：maxpooling、average pooling，可以降维。\n假设输入的尺寸是: $n\\times c_i\\times w_i\\times h_i$，分别表示batch_size,输入通道数，输入feature map的宽度和高度\n输出的尺寸是：$n\\times c_o\\times w_o\\times h_o$，分别表示batch_size,输出通道数，输出feature map的宽度和高度\n权重的形状：$c_i\\times c_o\\times k_w\\times k_h$，后两个是kernal的size，一般是（3，3），（5，5）\n高和宽一般相等，输出的尺寸可以计算为：\n$$ h_o=\\frac{h_i+2p-k}{s}+1 $$ 参数量计算 权重weihgt，即模型的大小，计算公式为： $$ c_o\\times c_i/g\\times k_w\\times k_h $$ 对于组卷积可以减少权重数量：\n$$ c_o\\times c_i/g\\times k_w\\times k_h $$ 激活activation，一般和所在层输出feature map大小相同\nMAC的数量,可以理解为输出的feature map大小乘上kernal的尺寸：\n$$ c_i\\times k_w\\times k_h\\times c_o\\times w_o\\times h_o $$ FLOPs和FLOPS\n前者表示总共的运算量，后者表示每秒的运算量\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","title":"Lec2：神经网络基础"},{"content":"Introduction 什么是剪枝 在保持模型性能的前提下，剪掉多余的神经元和权重,使模型变得更小\n剪枝的粒度 细粒度剪枝也叫非结构化剪枝，这种方式更灵活，能剪掉的东西更多，但硬件难以支持，剪了可能也无法提高硬件速度，粗粒度剪枝更规整，硬件友好，但剪枝比率小。\n下图越往右粒度越粗、剪枝比率越小、对硬件越友好\nFine-grained剪枝能找到更多冗余权重，但没有规律，只能在定制硬件上支持，大多数时候都没用。\nPattern-based 剪枝相对使用，如2：4结构化剪枝，在每四个连续的权重中剪掉两个。\nchannel级剪枝对硬件来说最直观，但剪枝比率也最小。\n剪枝的标准 剪枝剪掉的是相对不重要的神经元和权重，剪掉对模型影响最小的部分。\nMagnitude-based Pruning\n根据权重的绝对值剪枝，剪去绝对值较小的权重，或者根据权重的平方和。\nScaling-based Pruning\nchannel级剪枝，给每个输出通道乘一个scaling factor，作为参数在训练中优化，最后剪去较小的值对应的channel。\nSecond-Order-based Pruning\n最小化因为剪枝引起的损失函数的误差\nPercentage-of-Zero-based Pruning\n经过activation之后某个通道0的比例比较多就剪去\nRegression-based Pruning\n同样是最小化误差，选用的是剪枝层剪枝后的输出和未剪枝时的输出之间的误差\n剪枝率 均匀和非均匀 每一层对剪枝的敏感度不同，剪枝率也应该不相同\n如何找到合适的剪枝率 sensitivity analysis 对每一层进行敏感度分析，观察每一层单独剪枝比率对模型准确率的影响，然后设定一个准确率的阈值来确定每一层剪枝的剪枝率。\nAutomatic Pruning AMC:AutoML for Model Compression,把剪枝看作一个强化学习问题，自动寻找每一层合适的剪枝率,比手工寻找更有效。\n剪枝后的微调或重新训练 剪枝之后模型经过微调效果会恢复一些，一般微调的学习率设置的很小\n迭代剪枝 把剪枝和重新训练作为一轮迭代，迭代多轮剪枝率更大\n微调和训练中的regression 惩罚非零的参数，鼓励小参数\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec3%E5%89%AA%E6%9E%9D%E5%92%8C%E7%A8%80%E7%96%8F/","title":"Lec3：剪枝和稀疏"},{"content":"前言 由于想要做一个较为通用的AI芯片，因此需要支持尽可能多的激活函数，而非线性函数硬件实现比较复杂，如果为每一种非线性函数都定制硬件，浪费的资源太多，因此本文总结一下神经网络中常用的非线性函数以及可能的实现方式\nSoftmax $$ Softmax\\left( x_i \\right) =\\frac{e^{x_i}}{\\sum_{i=0}^n e^{x_j}} $$ 一般要考虑exp和除法的近似实现。\n可以参考两篇论文SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference和Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference，关于浮点除法的硬件近似和优化参考Approximate Integer and Floating-Point Dividers with Near-Zero Error Bias。\nLayernorm 用在attetion之前，对token进行归一化。对于一个输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，Layer Normalization 的公式如下：\n$$ \\mathrm{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mathrm{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}] + \\epsilon}} + \\beta $$其中：\n$\\mathrm{E}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的均值。 $\\mathrm{Var}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的方差。 $\\gamma$ 和 $\\beta$ 是可学习的参数，分别用于缩放和平移。 $\\epsilon$ 是一个小的常数，用来防止除以零。 在llama中使用的是RMSnorm，计算相对简单。\n对于同样的输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，RMS Normalization 的公式可以简化为：\n$$ \\mathrm{RMSNorm}(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} $$二者都需要计算倒数平方根，经典的倒数平方根算法见快速平方根倒数算法\nSigmoid $$ Sigmoid(x) = \\frac{1}{1 + e^{-x}} $$常用插值法实现，有一个比较有用的特性：\n$$ Sigmoid\\left( -x \\right) =1-Sigmoid\\left( x \\right) $$tanh 双曲正切函数，可以由sigmoid实现，公式为：\n$$ \\tanh \\left( x \\right) =\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid\\left( 2x \\right) -1 $$ReLU、Leaky ReLU等类ReLU函数 公式比较简单，就不写了，硬件实现也比较简单，没有特殊计算。\nSwish sigmoid基础上的一种复杂形式，公式为：\n$$ Swish\\left( x \\right) =x*Sigmoid\\left( x \\right) $$GeLU GELU 的公式有两种常见的形式：\n精确形式： $$ \\mathrm{GELU}(x) = x \\cdot \\Phi(x) $$其中 $\\Phi(x)$ 是标准正态分布的累积分布函数 (CDF)，可以表达为：\n$$ \\Phi(x) = \\frac{1}{2} \\left[ 1 + \\operatorname{erf} \\left( \\frac{x}{\\sqrt{2}} \\right) \\right] $$ 近似形式： $$ \\mathrm{GELU}(x) \\approx 0.5x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715x^3 \\right) \\right) \\right) $$SwiGLU 结合了Swish和GLU激活函数，SwiGLU 的公式如下：\n$$ \\mathrm{SwiGLU}(x, W, V, b, c) = (\\sigma(Wx + b) \\otimes (Vx + c)) $$其中：\n$\\sigma$ 表示 Sigmoid 函数。 $W$ 和 $V$ 是权重矩阵。 $b$ 和 $c$ 是偏置向量。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$x$分为两部分$x_1$和$x_2$（例如，在某些Transformer变体中），则SwiGLU可以写作：\n$$ \\mathrm{SwiGLU}(\\mathbf{x}) = \\sigma(\\mathbf{x}_1) \\otimes \\mathbf{x}_2 $$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是输入向量$\\mathbf{x}$的两个部分，通常通过线性变换得到。\nGeGLU 结合了GELU和GLU，GeGLU 的公式如下：\n$$ \\mathrm{GeGLU}(\\mathbf{x}) = \\left( \\mathrm{GELU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) \\right) \\otimes (\\mathbf{W}_2 \\mathbf{x} + \\mathbf{b}_2) $$其中：\n$\\mathbf{x}$ 是输入向量。 $\\mathbf{W}_1$ 和 $\\mathbf{W}_2$ 是权重矩阵。 $\\mathbf{b}_1$ 和 $\\mathbf{b}_2$ 是偏置向量。 $\\mathrm{GELU}(\\cdot)$ 表示 GELU 激活函数。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$\\mathbf{x}$通过线性变换分为两部分$\\mathbf{x}_1$和$\\mathbf{x}_2$，则GeGLU可以写作：\n$$ \\mathrm{GeGLU}(\\mathbf{x}) = \\mathrm{GELU}(\\mathbf{x}_1) \\otimes \\mathbf{x}_2 $$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是通过不同的线性变换从输入$\\mathbf{x}$得到的两个部分。在实际应用中，$\\mathbf{x}_1$通常用于计算门控信号，而$\\mathbf{x}_2$则是要被门控的信号。\n总结 想做通用还是尽量把exp，除法，sigmoid这种简单的做成专用电路，更复杂的激活函数也由这些算子实现\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/","title":"神经网络中的非线性函数"},{"content":"Hello World ","date":"2024-12-10T22:13:38+08:00","permalink":"https://mono-arch.github.io/p/myfirstblog/","title":"MyFirstBlog"}]