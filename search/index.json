[{"content":"Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 transforms = { \u0026#34;train\u0026#34;: Compose([ RandomCrop(32, padding=4), RandomHorizontalFlip(), ToTensor(), ]), \u0026#34;test\u0026#34;: ToTensor(), } dataset = {} # 下载cifar10数据集并处理 for split in [\u0026#34;train\u0026#34;, \u0026#34;test\u0026#34;]: dataset[split] = CIFAR10( root=\u0026#34;data/cifar10\u0026#34;, train=(split == \u0026#34;train\u0026#34;), download=True, transform=transforms[split], ) transforms把训练集和测试集的数据转换成tensor形式，是一个词典(dict),python中词典由key和value组成，这里train和test对应的数据处理方式不同，compose()会把括号中的多个数据操作组合在一起，顺序执行。\nRandomCrop随机剪裁图像为32*32，边缘padding是4，RandomHorizontalFlip随机水平翻转图像，这两个随机都会增强模型泛化能力。ToTensor是转换成tensor形式。\n1 2 3 4 5 6 7 8 9 dataflow = {} for split in [\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;]: dataflow[split] = DataLoader( dataset[split], batch_size=512, shuffle=(split == \u0026#39;train\u0026#39;), num_workers=0, pin_memory=True, ) 数据喂进模型的时候使用Dataloader来加载数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VGG(nn.Module): ARCH = [64, 128, \u0026#39;M\u0026#39;, 256, 256, \u0026#39;M\u0026#39;, 512, 512, \u0026#39;M\u0026#39;, 512, 512, \u0026#39;M\u0026#39;] def __init__(self) -\u0026gt; None: super().__init__() layers = [] counts = defaultdict(int) def add(name: str, layer: nn.Module) -\u0026gt; None: layers.append((f\u0026#34;{name}{counts[name]}\u0026#34;, layer)) counts[name] += 1 in_channels = 3 for x in self.ARCH: if x != \u0026#39;M\u0026#39;: # conv-bn-relu add(\u0026#34;conv\u0026#34;, nn.Conv2d(in_channels, x, 3, padding=1, bias=False)) add(\u0026#34;bn\u0026#34;, nn.BatchNorm2d(x)) add(\u0026#34;relu\u0026#34;, nn.ReLU(True)) in_channels = x else: # maxpool add(\u0026#34;pool\u0026#34;, nn.MaxPool2d(2)) self.backbone = nn.Sequential(OrderedDict(layers))#网络的主干部分就是每层串行连接起来 self.classifier = nn.Linear(512, 10)#输出层，由于是十分类输出就是10个元素 def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # backbone: [N, 3, 32, 32] =\u0026gt; [N, 512, 2, 2] x = self.backbone(x) # avgpool: [N, 512, 2, 2] =\u0026gt; [N, 512] x = x.mean([2, 3]) # classifier: [N, 512] =\u0026gt; [N, 10] x = self.classifier(x) return x model = VGG().cuda()#模型参数送到GPU里 这里选用的模型是VGG，ARCH里面是每一层输出通道数以及pooling层的位置，前向传播函数就是数据在网络中跑一遍\n","date":"2024-12-31T00:00:00Z","permalink":"https://mono-arch.github.io/p/lab0/","title":"Lab0"},{"content":"Post-Training Quantization 在线性量化中，如何得到最优的（S，Z）\n量化粒度 Per-Tensor Quantization\n直接把权重tensor作为一个整体量化，一层的权重只需要一个S和Z，对于较大的模型行得通，小模型掉精度。原因是不同channel的最大值差异很大，有一些channel有离群值。\nPer-Channel Quantization\n粒度相对更细，每个输出channel共享S和Z\nGroup Quantization\nPer-Vector Quantization\nShared Micro-exponent (MX) data type\n如下图所示，有两个scaling-factor，r是浮点型，一个tensor共享一个，Sq是整数型的，每个vector一个，粗粒度级使用更精细的。当然也可以采用其他分级方式，比如L0是每16个元素，L1是每channel\nDynamic Range Clipping 激活值的范围和输入有关，在部署模型之前需要确定激活值范围,激活值的范围太大会导致量化精度下降\nDuring training\n在训练的时候控制参数的大小，t时刻参数受t-1时刻的影响。\nrunning a few “calibration” batches\n跑校准数据集，一种方法是最小化量化前后的均方误差来确定最大激活值，另一种方法是通过最小化信息损失来确定，KL散度表征两个随机变量的关联程度。\nRounding 简单的四舍五入可能不是最优的，有时候需要自适应舍入。\nQuantization-Aware Training 模型在Post training quantization之后精度受影响较大，在量化之后需要微调或重新训练。\n由于量化的值是离散的，导数大部分都是0，反向传播更新不了权重，通过Straight-Through Estimator (STE)直接把对量化后的权重的偏导数作为权重的偏导数来更新权重。\nBinary/Ternary Quantization 二值化网络 权重量化成简单的-1和+1，这样权重只需要1bit来表示，也不需要乘法，只有加法。\n如果激活和权重都量化成二值，计算的时候只需要异或计算，不再有乘累加\n三值化网络 量化成-1，0，+1\n可训练的三值网络，wp和wn都是可训练的参数\nMixed-Precision Quantization 混合精度量化，每一层的权重和激活值都可以不同精度，这会导致整个网络的量化方式是一个很大的设计空间，需要自动搜索。\n","date":"2024-12-31T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec6-%E9%87%8F%E5%8C%96-ii/","title":"Lec6 :量化 II"},{"content":"数据类型 整数 有int4，int8，int16等。\n定点数 定点数是小数点固定的数，格式很灵活，例如下面是Q3.4定点数表示低4位表示小数部分，高3位表示整数部分，最高位是符号位。\n浮点数 常用的有fp8,fp16,bf16,fp32等，浮点数的精度不是均匀的，数越大精度越差。\n指数位宽决定了数据格式能表示的范围，尾数部分决定了数据格式的精度，如BF16在原来的fp16基础上指数位改成了8位，增加了数据表示范围，防止训练时数据的溢出。\n## 量化 三种量化形式如下\n貌似线性量化使用比较多\nK-Means-based Weight Quantization 这种量化方法使用K-Means算法把权重分成K个簇，簇的中心值作为量化后的代表值，用簇索引值代表权重，通过查找表找到量化的权重值。\n如下图所示，初始权重是32位float，通过K-Means划分成4个簇，index中存的是原始权重对应哪一个簇，codebook就是上述的查找表。\n下图是Alexnet经过剪枝和量化模型大小和精度的变化，可以看出压缩率还是非常可观的。\n这种量化只能减少weight存储容量，在计算中仍需要浮点形式的计算，需要查codebook获得计算时的权重。\nLinear Quantization 下图为线性量化的过程，Z是为了保证量化前的0等于量化后的0，S是Scale缩放比例。 计算使用的Z和S\n$$ S=\\frac{r_{\\max}-r_{\\min}}{q_{\\max}-q_{\\min}},Z=q_{\\min}-\\frac{r_{\\min}}{S} $$ 如图所示\n量化后的矩阵计算可以表示如下 可以看出量化后的计算步骤会比之前多，后两项是可以在输入数据之前预先计算的，假设是对称量化，前后的0本就对应相等，即Zw=0，那么计算可以进一步简化。\n量化后的全连接层计算可以表示如下 \u0026emsp;\u0026emsp;这里bias的量化参数要根据权重和输入的量化方式决定。 量化后的卷积计算可以表示如下 ","date":"2024-12-26T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec5-%E9%87%8F%E5%8C%96-i/","title":"Lec5 :量化 I"},{"content":"常用的神经网络层 全连接层 权重的数量：$c_i*c_o$，分别代表输入维度和输出维度\n卷积层 感受野 是指输出的一个像素由多少元素决定，越大说明涵盖的信息越全面\n参数 首先是一些概念，stride指卷积的步长，padding指边缘填充的元素数量，pooling：maxpooling、average pooling，可以降维。\n假设输入的尺寸是: $n\\times c_i\\times w_i\\times h_i$，分别表示batch_size,输入通道数，输入feature map的宽度和高度\n输出的尺寸是：$n\\times c_o\\times w_o\\times h_o$，分别表示batch_size,输出通道数，输出feature map的宽度和高度\n权重的形状：$c_i\\times c_o\\times k_w\\times k_h$，后两个是kernal的size，一般是（3，3），（5，5）\n高和宽一般相等，输出的尺寸可以计算为：\n$$ h_o=\\frac{h_i+2p-k}{s}+1 $$ 参数量计算 权重weihgt，即模型的大小，计算公式为： $$ c_o\\times c_i/g\\times k_w\\times k_h $$ 对于组卷积可以减少权重数量：\n$$ c_o\\times c_i/g\\times k_w\\times k_h $$ 激活activation，一般和所在层输出feature map大小相同\nMAC的数量,可以理解为输出的feature map大小乘上kernal的尺寸：\n$$ c_i\\times k_w\\times k_h\\times c_o\\times w_o\\times h_o $$ FLOPs和FLOPS\n前者表示总共的运算量，后者表示每秒的运算量\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","title":"Lec2：神经网络基础"},{"content":"Introduction 什么是剪枝 在保持模型性能的前提下，剪掉多余的神经元和权重,使模型变得更小\n剪枝的粒度 细粒度剪枝也叫非结构化剪枝，这种方式更灵活，能剪掉的东西更多，但硬件难以支持，剪了可能也无法提高硬件速度，粗粒度剪枝更规整，硬件友好，但剪枝比率小。\n下图越往右粒度越粗、剪枝比率越小、对硬件越友好\nFine-grained剪枝能找到更多冗余权重，但没有规律，只能在定制硬件上支持，大多数时候都没用。\nPattern-based 剪枝相对使用，如2：4结构化剪枝，在每四个连续的权重中剪掉两个。\nchannel级剪枝对硬件来说最直观，但剪枝比率也最小。\n剪枝的标准 剪枝剪掉的是相对不重要的神经元和权重，剪掉对模型影响最小的部分。\nMagnitude-based Pruning 根据权重的绝对值剪枝，剪去绝对值较小的权重，或者根据权重的平方和。\nScaling-based Pruning channel级剪枝，给每个输出通道乘一个scaling factor，作为参数在训练中优化，最后剪去较小的值对应的channel。\nSecond-Order-based Pruning 最小化因为剪枝引起的损失函数的误差\nPercentage-of-Zero-based Pruning 经过activation之后某个通道0的比例比较多就剪去\nRegression-based Pruning 同样是最小化误差，选用的是剪枝层剪枝后的输出和未剪枝时的输出之间的误差\n剪枝率 均匀和非均匀 每一层对剪枝的敏感度不同，剪枝率也应该不相同\n如何找到合适的剪枝率 sensitivity analysis 对每一层进行敏感度分析，观察每一层单独剪枝比率对模型准确率的影响，然后设定一个准确率的阈值来确定每一层剪枝的剪枝率。\nAutomatic Pruning AMC:AutoML for Model Compression,把剪枝看作一个强化学习问题，自动寻找每一层合适的剪枝率,比手工寻找更有效。\n剪枝后的微调或重新训练 剪枝之后模型经过微调效果会恢复一些，一般微调的学习率设置的很小\n迭代剪枝 把剪枝和重新训练作为一轮迭代，迭代多轮剪枝率更大\n微调和训练中的regression 惩罚非零的参数，鼓励小参数\nSystem \u0026amp; Hardware Support for Sparsity EIE:weight和activation都稀疏 TensorCore：加入了2:4的结构化稀疏 ","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/lec34%E5%89%AA%E6%9E%9D%E5%92%8C%E7%A8%80%E7%96%8F/","title":"Lec3,4：剪枝和稀疏"},{"content":"前言 由于想要做一个较为通用的AI芯片，因此需要支持尽可能多的激活函数，而非线性函数硬件实现比较复杂，如果为每一种非线性函数都定制硬件，浪费的资源太多，因此本文总结一下神经网络中常用的非线性函数以及可能的实现方式\nSoftmax $$ Softmax\\left( x_i \\right) =\\frac{e^{x_i}}{\\sum_{i=0}^n e^{x_j}} $$ 一般要考虑exp和除法的近似实现。\n可以参考两篇论文SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference和Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference，关于浮点除法的硬件近似和优化参考Approximate Integer and Floating-Point Dividers with Near-Zero Error Bias。\nLayernorm 用在attetion之前，对token进行归一化。对于一个输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，Layer Normalization 的公式如下：\n$$ \\mathrm{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mathrm{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}] + \\epsilon}} + \\beta $$其中：\n$\\mathrm{E}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的均值。 $\\mathrm{Var}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的方差。 $\\gamma$ 和 $\\beta$ 是可学习的参数，分别用于缩放和平移。 $\\epsilon$ 是一个小的常数，用来防止除以零。 在llama中使用的是RMSnorm，计算相对简单。\n对于同样的输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，RMS Normalization 的公式可以简化为：\n$$ \\mathrm{RMSNorm}(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} $$二者都需要计算倒数平方根，经典的倒数平方根算法见快速平方根倒数算法\nSigmoid $$ Sigmoid(x) = \\frac{1}{1 + e^{-x}} $$常用插值法实现，有一个比较有用的特性：\n$$ Sigmoid\\left( -x \\right) =1-Sigmoid\\left( x \\right) $$tanh 双曲正切函数，可以由sigmoid实现，公式为：\n$$ \\tanh \\left( x \\right) =\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid\\left( 2x \\right) -1 $$ReLU、Leaky ReLU等类ReLU函数 公式比较简单，就不写了，硬件实现也比较简单，没有特殊计算。\nSwish sigmoid基础上的一种复杂形式，公式为：\n$$ Swish\\left( x \\right) =x\\times Sigmoid\\left( x \\right) $$GeLU GELU 的公式有两种常见的形式：\n精确形式： $$ \\mathrm{GELU}(x) = x \\cdot \\Phi(x) $$其中 $\\Phi(x)$ 是标准正态分布的累积分布函数 (CDF)，可以表达为：\n$$ \\Phi(x) = \\frac{1}{2} \\left[ 1 + \\operatorname{erf} \\left( \\frac{x}{\\sqrt{2}} \\right) \\right] $$ 近似形式： $$ \\mathrm{GELU}(x) \\approx 0.5x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715x^3 \\right) \\right) \\right) $$$$ GELU\\left( x \\right) \\approx x\\times Sigmoid\\left( 1.702\\times x \\right) $$SwiGLU 结合了Swish和GLU激活函数，SwiGLU 的公式如下：\n$$ \\mathrm{SwiGLU}(x, W, V, b, c) = (\\sigma(Wx + b) \\otimes (Vx + c)) $$其中：\n$\\sigma$ 表示 Sigmoid 函数。 $W$ 和 $V$ 是权重矩阵。 $b$ 和 $c$ 是偏置向量。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$x$分为两部分$x_1$和$x_2$（例如，在某些Transformer变体中），则SwiGLU可以写作：\n$$ \\mathrm{SwiGLU}(\\mathbf{x}) = \\sigma(\\mathbf{x}_1) \\otimes \\mathbf{x}_2 $$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是输入向量$\\mathbf{x}$的两个部分，通常通过线性变换得到。\nGeGLU 结合了GELU和GLU，GeGLU 的公式如下：\n$$ \\mathrm{GeGLU}(\\mathbf{x}) = \\left( \\mathrm{GELU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) \\right) \\otimes (\\mathbf{W}_2 \\mathbf{x} + \\mathbf{b}_2) $$其中：\n$\\mathbf{x}$ 是输入向量。 $\\mathbf{W}_1$ 和 $\\mathbf{W}_2$ 是权重矩阵。 $\\mathbf{b}_1$ 和 $\\mathbf{b}_2$ 是偏置向量。 $\\mathrm{GELU}(\\cdot)$ 表示 GELU 激活函数。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$\\mathbf{x}$通过线性变换分为两部分$\\mathbf{x}_1$和$\\mathbf{x}_2$，则GeGLU可以写作：\n$$ \\mathrm{GeGLU}(\\mathbf{x}) = \\mathrm{GELU}(\\mathbf{x}_1) \\otimes \\mathbf{x}_2 $$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是通过不同的线性变换从输入$\\mathbf{x}$得到的两个部分。在实际应用中，$\\mathbf{x}_1$通常用于计算门控信号，而$\\mathbf{x}_2$则是要被门控的信号。\n总结 想做通用还是尽量把exp，除法，sigmoid这种简单的做成专用电路，更复杂的激活函数也由这些算子实现\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/","title":"神经网络中的非线性函数"},{"content":"Hello World ","date":"2024-12-10T22:13:38+08:00","permalink":"https://mono-arch.github.io/p/myfirstblog/","title":"MyFirstBlog"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://mono-arch.github.io/p/","title":""}]