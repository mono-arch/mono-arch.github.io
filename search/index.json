[{"content":"前言 由于想要做一个较为通用的AI芯片，因此需要支持尽可能多的激活函数，而非线性函数硬件实现比较复杂，如果为每一种非线性函数都定制硬件，浪费的资源太多，因此本文总结一下神经网络中常用的非线性函数以及可能的实现方式\nSoftmax $$\rSoftmax\\left( x_i \\right) =\\frac{e^{x_i}}{\\sum_{i=0}^n e^{x_j}}\r$$ 一般要考虑exp和除法的近似实现。\n可以参考两篇论文SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference和Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference，关于浮点除法的硬件近似和优化参考Approximate Integer and Floating-Point Dividers with Near-Zero Error Bias。\nLayernorm 用在attetion之前，对token进行归一化。对于一个输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，Layer Normalization 的公式如下：\n$$\r\\mathrm{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mathrm{E}[\\mathbf{x}]}{\\sqrt{\\mathrm{Var}[\\mathbf{x}] + \\epsilon}} + \\beta\r$$其中：\n$\\mathrm{E}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的均值。 $\\mathrm{Var}[\\mathbf{x}]$ 是 $\\mathbf{x}$ 的方差。 $\\gamma$ 和 $\\beta$ 是可学习的参数，分别用于缩放和平移。 $\\epsilon$ 是一个小的常数，用来防止除以零。 在llama中使用的是RMSnorm，计算相对简单。\n对于同样的输入向量 $\\mathbf{x} = [x_1, x_2, \u0026hellip;, x_n]$，RMS Normalization 的公式可以简化为：\n$$\r\\mathrm{RMSNorm}(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}}\r$$二者都需要计算倒数平方根，经典的倒数平方根算法见快速平方根倒数算法\nSigmoid $$\rSigmoid(x) = \\frac{1}{1 + e^{-x}}\r$$常用插值法实现，有一个比较有用的特性：\n$$\rSigmoid\\left( -x \\right) =1-Sigmoid\\left( x \\right) $$tanh 双曲正切函数，可以由sigmoid实现，公式为：\n$$\r\\tanh \\left( x \\right) =\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid\\left( 2x \\right) -1\r$$ReLU、Leaky ReLU等类ReLU函数 公式比较简单，就不写了，硬件实现也比较简单，没有特殊计算。\nSwish sigmoid基础上的一种复杂形式，公式为：\n$$\rSwish\\left( x \\right) =x*Sigmoid\\left( x \\right) $$GeLU GELU 的公式有两种常见的形式：\n精确形式： $$\r\\mathrm{GELU}(x) = x \\cdot \\Phi(x)\r$$其中 $\\Phi(x)$ 是标准正态分布的累积分布函数 (CDF)，可以表达为：\n$$\r\\Phi(x) = \\frac{1}{2} \\left[ 1 + \\operatorname{erf} \\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\r$$ 近似形式： $$\r\\mathrm{GELU}(x) \\approx 0.5x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715x^3 \\right) \\right) \\right)\r$$SwiGLU 结合了Swish和GLU激活函数，SwiGLU 的公式如下：\n$$\r\\mathrm{SwiGLU}(x, W, V, b, c) = (\\sigma(Wx + b) \\otimes (Vx + c))\r$$其中：\n$\\sigma$ 表示 Sigmoid 函数。 $W$ 和 $V$ 是权重矩阵。 $b$ 和 $c$ 是偏置向量。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$x$分为两部分$x_1$和$x_2$（例如，在某些Transformer变体中），则SwiGLU可以写作：\n$$\r\\mathrm{SwiGLU}(\\mathbf{x}) = \\sigma(\\mathbf{x}_1) \\otimes \\mathbf{x}_2\r$$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是输入向量$\\mathbf{x}$的两个部分，通常通过线性变换得到。\nGeGLU 结合了GELU和GLU，GeGLU 的公式如下：\n$$\r\\mathrm{GeGLU}(\\mathbf{x}) = \\left( \\mathrm{GELU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) \\right) \\otimes (\\mathbf{W}_2 \\mathbf{x} + \\mathbf{b}_2)\r$$其中：\n$\\mathbf{x}$ 是输入向量。 $\\mathbf{W}_1$ 和 $\\mathbf{W}_2$ 是权重矩阵。 $\\mathbf{b}_1$ 和 $\\mathbf{b}_2$ 是偏置向量。 $\\mathrm{GELU}(\\cdot)$ 表示 GELU 激活函数。 $\\otimes$ 表示元素级别的乘法。 更具体地，如果我们将输入$\\mathbf{x}$通过线性变换分为两部分$\\mathbf{x}_1$和$\\mathbf{x}_2$，则GeGLU可以写作：\n$$\r\\mathrm{GeGLU}(\\mathbf{x}) = \\mathrm{GELU}(\\mathbf{x}_1) \\otimes \\mathbf{x}_2\r$$这里$\\mathbf{x}_1$和$\\mathbf{x}_2$是通过不同的线性变换从输入$\\mathbf{x}$得到的两个部分。在实际应用中，$\\mathbf{x}_1$通常用于计算门控信号，而$\\mathbf{x}_2$则是要被门控的信号。\n总结 想做通用还是尽量把exp，除法，sigmoid这种简单的做成专用电路，更复杂的激活函数也由这些算子实现\n","date":"2024-12-13T00:00:00Z","permalink":"https://mono-arch.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/","title":"神经网络中的非线性函数"},{"content":"Hello World ","date":"2024-12-10T22:13:38+08:00","permalink":"https://mono-arch.github.io/p/myfirstblog/","title":"MyFirstBlog"}]