<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on ggk</title>
        <link>http://localhost:1313/zh-cn/post/</link>
        <description>Recent content in Posts on ggk</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>ggk</copyright>
        <lastBuildDate>Fri, 13 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/zh-cn/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>神经网络中的非线性函数</title>
        <link>http://localhost:1313/zh-cn/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/</link>
        <pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/zh-cn/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/</guid>
        <description>&lt;h1 id=&#34;前言&#34;&gt;前言
&lt;/h1&gt;&lt;p&gt;由于想要做一个较为通用的AI芯片，因此需要支持尽可能多的激活函数，而非线性函数硬件实现比较复杂，如果为每一种非线性函数都定制硬件，浪费的资源太多，因此本文总结一下神经网络中常用的非线性函数以及可能的实现方式&lt;/p&gt;
&lt;h2 id=&#34;softmax&#34;&gt;Softmax
&lt;/h2&gt;$$
Softmax\left( x_i \right) =\frac{e^{x_i}}{\sum_{i=0}^n e^{x_j}}
$$&lt;p&gt;
一般要考虑exp和除法的近似实现。&lt;br&gt;
可以参考两篇论文&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/10323725&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SOLE: Hardware-Software Co-design of Softmax  and LayerNorm for Efficient Transformer Inference&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/abs/10.1145/3665314.3670816&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference&lt;/a&gt;，关于浮点除法的硬件近似和优化参考&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/abs/10.1145/3316781.3317773&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Approximate Integer and Floating-Point Dividers with Near-Zero Error Bias&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;layernorm&#34;&gt;Layernorm
&lt;/h2&gt;&lt;p&gt;用在attetion之前，对token进行归一化。对于一个输入向量 $\mathbf{x} = [x_1, x_2, &amp;hellip;, x_n]$，Layer Normalization 的公式如下：&lt;/p&gt;
$$
\mathrm{LayerNorm}(\mathbf{x}) = \gamma \cdot \frac{\mathbf{x} - \mathrm{E}[\mathbf{x}]}{\sqrt{\mathrm{Var}[\mathbf{x}] + \epsilon}} + \beta
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathrm{E}[\mathbf{x}]$ 是 $\mathbf{x}$ 的均值。&lt;/li&gt;
&lt;li&gt;$\mathrm{Var}[\mathbf{x}]$ 是 $\mathbf{x}$ 的方差。&lt;/li&gt;
&lt;li&gt;$\gamma$ 和 $\beta$ 是可学习的参数，分别用于缩放和平移。&lt;/li&gt;
&lt;li&gt;$\epsilon$ 是一个小的常数，用来防止除以零。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在llama中使用的是RMSnorm，计算相对简单。&lt;/p&gt;
&lt;p&gt;对于同样的输入向量 $\mathbf{x} = [x_1, x_2, &amp;hellip;, x_n]$，RMS Normalization 的公式可以简化为：&lt;/p&gt;
$$
\mathrm{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}}
$$&lt;p&gt;二者都需要计算倒数平方根，经典的倒数平方根算法见&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/638013235&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;快速平方根倒数算法&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;sigmoid&#34;&gt;Sigmoid
&lt;/h2&gt;$$
Sigmoid(x) = \frac{1}{1 + e^{-x}}
$$&lt;p&gt;常用插值法实现，有一个比较有用的特性：&lt;/p&gt;
$$
Sigmoid\left( -x \right) =1-Sigmoid\left( x \right) 
$$&lt;h2 id=&#34;tanh&#34;&gt;tanh
&lt;/h2&gt;&lt;p&gt;双曲正切函数，可以由sigmoid实现，公式为：&lt;/p&gt;
$$
\tanh \left( x \right) =\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid\left( 2x \right) -1
$$&lt;h2 id=&#34;reluleaky-relu等类relu函数&#34;&gt;ReLU、Leaky ReLU等类ReLU函数
&lt;/h2&gt;&lt;p&gt;公式比较简单，就不写了，硬件实现也比较简单，没有特殊计算。&lt;/p&gt;
&lt;h2 id=&#34;swish&#34;&gt;Swish
&lt;/h2&gt;&lt;p&gt;sigmoid基础上的一种复杂形式，公式为：&lt;/p&gt;
$$
Swish\left( x \right) =x*Sigmoid\left( x \right) 
$$&lt;h2 id=&#34;gelu&#34;&gt;GeLU
&lt;/h2&gt;&lt;p&gt;GELU 的公式有两种常见的形式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;精确形式&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
$$
\mathrm{GELU}(x) = x \cdot \Phi(x)
$$&lt;p&gt;其中 $\Phi(x)$ 是标准正态分布的累积分布函数 (CDF)，可以表达为：&lt;/p&gt;
$$
\Phi(x) = \frac{1}{2} \left[ 1 + \operatorname{erf} \left( \frac{x}{\sqrt{2}} \right) \right]
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;近似形式&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
$$
\mathrm{GELU}(x) \approx 0.5x \left( 1 + \tanh \left( \sqrt{\frac{2}{\pi}} \left( x + 0.044715x^3 \right) \right) \right)
$$&lt;h2 id=&#34;swiglu&#34;&gt;SwiGLU
&lt;/h2&gt;&lt;p&gt;结合了Swish和GLU激活函数，SwiGLU 的公式如下：&lt;/p&gt;
$$
\mathrm{SwiGLU}(x, W, V, b, c) = (\sigma(Wx + b) \otimes (Vx + c))
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sigma$ 表示 Sigmoid 函数。&lt;/li&gt;
&lt;li&gt;$W$ 和 $V$ 是权重矩阵。&lt;/li&gt;
&lt;li&gt;$b$ 和 $c$ 是偏置向量。&lt;/li&gt;
&lt;li&gt;$\otimes$ 表示元素级别的乘法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更具体地，如果我们将输入$x$分为两部分$x_1$和$x_2$（例如，在某些Transformer变体中），则SwiGLU可以写作：&lt;/p&gt;
$$
\mathrm{SwiGLU}(\mathbf{x}) = \sigma(\mathbf{x}_1) \otimes \mathbf{x}_2
$$&lt;p&gt;这里$\mathbf{x}_1$和$\mathbf{x}_2$是输入向量$\mathbf{x}$的两个部分，通常通过线性变换得到。&lt;/p&gt;
&lt;h2 id=&#34;geglu&#34;&gt;GeGLU
&lt;/h2&gt;&lt;p&gt;结合了GELU和GLU，GeGLU 的公式如下：&lt;/p&gt;
$$
\mathrm{GeGLU}(\mathbf{x}) = \left( \mathrm{GELU}(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \right) \otimes (\mathbf{W}_2 \mathbf{x} + \mathbf{b}_2)
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$ 是输入向量。&lt;/li&gt;
&lt;li&gt;$\mathbf{W}_1$ 和 $\mathbf{W}_2$ 是权重矩阵。&lt;/li&gt;
&lt;li&gt;$\mathbf{b}_1$ 和 $\mathbf{b}_2$ 是偏置向量。&lt;/li&gt;
&lt;li&gt;$\mathrm{GELU}(\cdot)$ 表示 GELU 激活函数。&lt;/li&gt;
&lt;li&gt;$\otimes$ 表示元素级别的乘法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更具体地，如果我们将输入$\mathbf{x}$通过线性变换分为两部分$\mathbf{x}_1$和$\mathbf{x}_2$，则GeGLU可以写作：&lt;/p&gt;
$$
\mathrm{GeGLU}(\mathbf{x}) = \mathrm{GELU}(\mathbf{x}_1) \otimes \mathbf{x}_2
$$&lt;p&gt;这里$\mathbf{x}_1$和$\mathbf{x}_2$是通过不同的线性变换从输入$\mathbf{x}$得到的两个部分。在实际应用中，$\mathbf{x}_1$通常用于计算门控信号，而$\mathbf{x}_2$则是要被门控的信号。&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;想做通用还是尽量把exp，除法，sigmoid这种简单的做成专用电路，更复杂的激活函数也由这些算子实现&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Chinese Test</title>
        <link>http://localhost:1313/zh-cn/p/test-chinese/</link>
        <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/zh-cn/p/test-chinese/</guid>
        <description>&lt;img src="http://localhost:1313/helena-hertz-wWZzXlDpMog-unsplash.jpg" alt="Featured image of post Chinese Test" /&gt;&lt;h2 id=&#34;正文测试&#34;&gt;正文测试
&lt;/h2&gt;&lt;p&gt;而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。&lt;/p&gt;
&lt;p&gt;奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。&lt;/p&gt;
&lt;h2 id=&#34;引用&#34;&gt;引用
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;思念是最暖的忧伤像一双翅膀&lt;br&gt;
让我停不了飞不远在过往游荡&lt;br&gt;
不告而别的你 就算为了我着想&lt;br&gt;
这么沉痛的呵护 我怎么能翱翔&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=3aypp_YlBzI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;最暖的憂傷 - 田馥甄&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;图片&#34;&gt;图片
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/florian-klauer-nptLmg6jqDo-unsplash.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Florian Klauer on Unsplash&#34;
	
	
&gt;  &lt;img src=&#34;http://localhost:1313/luca-bravo-alS7ewQ41M8-unsplash.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Luca Bravo on Unsplash&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/helena-hertz-wWZzXlDpMog-unsplash.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Helena Hertz on Unsplash&#34;
	
	
&gt;  &lt;img src=&#34;http://localhost:1313/hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Hudai Gayiran on Unsplash&#34;
	
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;![&lt;span class=&#34;nt&#34;&gt;Photo by Florian Klauer on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;florian-klauer-nptLmg6jqDo-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Luca Bravo on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;luca-bravo-alS7ewQ41M8-unsplash.jpg&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;![&lt;span class=&#34;nt&#34;&gt;Photo by Helena Hertz on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;helena-hertz-wWZzXlDpMog-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Hudai Gayiran on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;相册语法来自 &lt;a class=&#34;link&#34; href=&#34;https://typlog.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Typlog&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
